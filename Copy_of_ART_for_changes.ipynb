{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e064e535bf5946cab16304204e836a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53d35d7ddff44de6a6e01aab40006132",
              "IPY_MODEL_aeade00b7f4440c1a223a4c423b29c61",
              "IPY_MODEL_ee63901b071842fc91a47717211bfe82"
            ],
            "layout": "IPY_MODEL_13f4ee859a904b82a890c72e7134f8c8"
          }
        },
        "53d35d7ddff44de6a6e01aab40006132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e49efe9dcf214bd084f9cf3bb332b7f3",
            "placeholder": "​",
            "style": "IPY_MODEL_484d9a6442cf4616acabe308c03257c4",
            "value": "100%"
          }
        },
        "aeade00b7f4440c1a223a4c423b29c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab8af23efb9b4c55ba5542ae8186046b",
            "max": 377664473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2707c12b6eed4658a439a4531d07ebed",
            "value": 377664473
          }
        },
        "ee63901b071842fc91a47717211bfe82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ac0406046134c5ab02417aad725e632",
            "placeholder": "​",
            "style": "IPY_MODEL_49f29a5cd947415680e9f65d3546c132",
            "value": " 360M/360M [00:01&lt;00:00, 209MB/s]"
          }
        },
        "13f4ee859a904b82a890c72e7134f8c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e49efe9dcf214bd084f9cf3bb332b7f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "484d9a6442cf4616acabe308c03257c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab8af23efb9b4c55ba5542ae8186046b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2707c12b6eed4658a439a4531d07ebed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ac0406046134c5ab02417aad725e632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f29a5cd947415680e9f65d3546c132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hammaad2002/ARTToolbox/blob/main/Copy_of_ART_for_changes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# shutil.rmtree(\"/content/adversarial-robustness-toolbox/\")\n",
        "# shutil.rmtree(\"/content/deepspeech.pytorch/\")\n",
        "# shutil.rmtree(\"/content/ARTToolbox/\")"
      ],
      "metadata": {
        "id": "atnW-aYtOAY8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/"
      ],
      "metadata": {
        "id": "DiZ_CEWbPPhl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Trusted-AI/adversarial-robustness-toolbox.git\n",
        "!cd adversarial-robustness-toolbox\n",
        "!pip install -e "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLL1Vt9_XnXc",
        "outputId": "fafc0a6e-7214-4cc0-8e70-873dd9ce8e9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'adversarial-robustness-toolbox'...\n",
            "remote: Enumerating objects: 75252, done.\u001b[K\n",
            "remote: Counting objects: 100% (932/932), done.\u001b[K\n",
            "remote: Compressing objects: 100% (350/350), done.\u001b[K\n",
            "remote: Total 75252 (delta 681), reused 774 (delta 580), pack-reused 74320\u001b[K\n",
            "Receiving objects: 100% (75252/75252), 400.79 MiB | 32.55 MiB/s, done.\n",
            "Resolving deltas: 100% (59563/59563), done.\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "-e option requires 1 argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1:  line 149 <class 'pytorch_lightning.core.module.LightningModule'>\n",
        "\n",
        "STEP 2: line 218 delete use_half\n",
        "\n",
        "STEP 3: line 384,458,569,628 outputs, output_sizes, *_ = self._model(inputs.to(self._device), input_sizes.to(self._device))\n",
        "\n",
        "Change file ImperceptableASR file:\n",
        "\n",
        "STEP 4: line 118 masker = PsychoacousticMasker()\n",
        "\n",
        "STEP 5: line 121 self.masker = masker"
      ],
      "metadata": {
        "id": "k0JSUcUtoOV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "my_string = \"\"\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    global my_string\n",
        "    if b.description == 'Colab':\n",
        "        my_string = '/content/'\n",
        "        other_button = button_kaggle\n",
        "    else:\n",
        "        my_string = '/kaggle/working/'\n",
        "        other_button = button_colab\n",
        "    print(my_string, \"will be the path !\")\n",
        "    b.disabled = True\n",
        "    other_button.disabled = True\n",
        "    \n",
        "button_colab = widgets.Button(description='Colab')\n",
        "button_kaggle = widgets.Button(description='Kaggle')\n",
        "\n",
        "button_colab.on_click(on_button_clicked)\n",
        "button_kaggle.on_click(on_button_clicked)\n",
        "\n",
        "display(widgets.HBox([button_colab, button_kaggle]))'''"
      ],
      "metadata": {
        "id": "HksodOHnTjbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "6d72c07f-c636-49df-d571-1c77734791a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import ipywidgets as widgets\\nfrom IPython.display import display, HTML\\n\\nmy_string = \"\"\\n\\ndef on_button_clicked(b):\\n    global my_string\\n    if b.description == \\'Colab\\':\\n        my_string = \\'/content/\\'\\n        other_button = button_kaggle\\n    else:\\n        my_string = \\'/kaggle/working/\\'\\n        other_button = button_colab\\n    print(my_string, \"will be the path !\")\\n    b.disabled = True\\n    other_button.disabled = True\\n    \\nbutton_colab = widgets.Button(description=\\'Colab\\')\\nbutton_kaggle = widgets.Button(description=\\'Kaggle\\')\\n\\nbutton_colab.on_click(on_button_clicked)\\nbutton_kaggle.on_click(on_button_clicked)\\n\\ndisplay(widgets.HBox([button_colab, button_kaggle]))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"hammaad2002\"\n",
        "!git config --global user.email \"hammadalikhan2002@gmail.com\"\n",
        "!git config --global user.password \"6Xp5bq@WRDUuU2W\""
      ],
      "metadata": {
        "id": "-coXlkoQNqXY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = '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' #this token will expire on Sat, Apr 8 2023\n",
        "username = 'hammaad2002'\n",
        "repo = 'ARTToolbox'"
      ],
      "metadata": {
        "id": "aDhzN97aOlYn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global --unset http.proxy"
      ],
      "metadata": {
        "id": "n4zvLPYAOm6t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "id": "uV-A2wjBOpe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c05abb4-ac54-4e1f-92e4-585e61d0ca3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ARTToolbox'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 108 (delta 62), reused 70 (delta 31), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (108/108), 567.60 KiB | 5.51 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = '/content/'"
      ],
      "metadata": {
        "id": "XMa5vLoYV3CJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,shutil\n",
        "os.remove(my_string + \"adversarial-robustness-toolbox/art/attacks/evasion/imperceptible_asr/imperceptible_asr_pytorch.py\")\n",
        "shutil.copy(my_string + 'ARTToolbox/imperceptible_asr_pytorch.py', '/content/adversarial-robustness-toolbox/art/attacks/evasion/imperceptible_asr/')\n",
        "os.remove(my_string + \"adversarial-robustness-toolbox/art/estimators/speech_recognition/pytorch_deep_speech.py\")\n",
        "shutil.copy(my_string + 'ARTToolbox/pytorch_deep_speech.py','/content/adversarial-robustness-toolbox/art/estimators/speech_recognition/')\n",
        "shutil.copy(my_string + 'ARTToolbox/wav2vec2ModelWrapper.py','/content/adversarial-robustness-toolbox/art/estimators/speech_recognition/')"
      ],
      "metadata": {
        "id": "AyxDc3vlP8tM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bfca0dea-ea20-4816-8cfa-8bf5063bf539"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/adversarial-robustness-toolbox/art/estimators/speech_recognition/wav2vec2ModelWrapper.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(my_string + \"adversarial-robustness-toolbox\")"
      ],
      "metadata": {
        "id": "A73vqNdGXFTC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ART TOOLBOX instructions:**\n",
        "When installling ART toolbox do the changes given below in the art/estimator/speech_recognition/pytorch_deepspeech file or else the code won't run. First install the toolbox then navigate through the files and make changes before running all the cell in top to bottom manner."
      ],
      "metadata": {
        "id": "UZLXmDP0hYDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/"
      ],
      "metadata": {
        "id": "yz5FlAaTmnVk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b main https://github.com/hammaad2002/CRDNN_Model.git"
      ],
      "metadata": {
        "id": "LJRa-2zWoJ23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1310f3ba-1cd2-4175-c928-308a0682cc49"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CRDNN_Model'...\n",
            "remote: Enumerating objects: 130, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/24)\u001b[K\rremote: Counting objects:   8% (2/24)\u001b[K\rremote: Counting objects:  12% (3/24)\u001b[K\rremote: Counting objects:  16% (4/24)\u001b[K\rremote: Counting objects:  20% (5/24)\u001b[K\rremote: Counting objects:  25% (6/24)\u001b[K\rremote: Counting objects:  29% (7/24)\u001b[K\rremote: Counting objects:  33% (8/24)\u001b[K\rremote: Counting objects:  37% (9/24)\u001b[K\rremote: Counting objects:  41% (10/24)\u001b[K\rremote: Counting objects:  45% (11/24)\u001b[K\rremote: Counting objects:  50% (12/24)\u001b[K\rremote: Counting objects:  54% (13/24)\u001b[K\rremote: Counting objects:  58% (14/24)\u001b[K\rremote: Counting objects:  62% (15/24)\u001b[K\rremote: Counting objects:  66% (16/24)\u001b[K\rremote: Counting objects:  70% (17/24)\u001b[K\rremote: Counting objects:  75% (18/24)\u001b[K\rremote: Counting objects:  79% (19/24)\u001b[K\rremote: Counting objects:  83% (20/24)\u001b[K\rremote: Counting objects:  87% (21/24)\u001b[K\rremote: Counting objects:  91% (22/24)\u001b[K\rremote: Counting objects:  95% (23/24)\u001b[K\rremote: Counting objects: 100% (24/24)\u001b[K\rremote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 130 (delta 8), reused 9 (delta 2), pack-reused 106\u001b[K\n",
            "Receiving objects: 100% (130/130), 48.94 MiB | 36.93 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate(x: ndarray, y: Optional[ndarray] = None, **kwargs)→ ndarray"
      ],
      "metadata": {
        "id": "ho3Ah9ql0r4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd"
      ],
      "metadata": {
        "id": "9zsnBXHdfK41"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **------ATTACK IMPLEMENTATION ON DEEPSPEECH MODEL USING ART TOOLBOX------**"
      ],
      "metadata": {
        "id": "G-k_XWrufYds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **----------------------ATTACK PREPROCESSING AND INITIALIZATION----------------------**"
      ],
      "metadata": {
        "id": "uLVipigqgrRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from art.estimators.speech_recognition.wav2vec2ModelWrapper import wav2vec2Model\n",
        "from art.attacks.evasion import FastGradientMethod , BasicIterativeMethod, ProjectedGradientDescent, ImperceptibleASRPyTorch, CarliniWagnerASR\n",
        "import numpy as np\n",
        "import librosa"
      ],
      "metadata": {
        "id": "MErAZ7pjfNDH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "aik7C1kXJtUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--------------------------------- wav2vec domain ------------------------------**"
      ],
      "metadata": {
        "id": "CL9JUdoOJyUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)"
      ],
      "metadata": {
        "id": "OrnJBuS-KfrC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "e064e535bf5946cab16304204e836a31",
            "53d35d7ddff44de6a6e01aab40006132",
            "aeade00b7f4440c1a223a4c423b29c61",
            "ee63901b071842fc91a47717211bfe82",
            "13f4ee859a904b82a890c72e7134f8c8",
            "e49efe9dcf214bd084f9cf3bb332b7f3",
            "484d9a6442cf4616acabe308c03257c4",
            "ab8af23efb9b4c55ba5542ae8186046b",
            "2707c12b6eed4658a439a4531d07ebed",
            "2ac0406046134c5ab02417aad725e632",
            "49f29a5cd947415680e9f65d3546c132"
          ]
        },
        "outputId": "9588077a-e8a5-4641-cdcb-f4ef578ef6f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/360M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e064e535bf5946cab16304204e836a31"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqQVST6OXmzT",
        "outputId": "6eca1410-4c6d-4315-9f78-4c8b8e4f453d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchaudio.models.wav2vec2.model.Wav2Vec2Model"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "TFrdwPx_JvrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load some audio data for testing\n",
        "x, sr = librosa.load( my_string + \"CRDNN_Model/AudioSamplesASR/spk1_snt1.wav\" , sr=None )\n",
        "y = np.array([ 'THE CHILD ALMOST HURT THE BIG DOG' ])\n",
        "y = y.reshape( 1 , 1 )\n",
        "print( y.shape )\n",
        "x = torch.from_numpy( x )\n",
        "x = x.unsqueeze( 0 ) \n",
        "x = x.numpy()\n",
        "print( x.shape )\n",
        "y1 =  np.array(['THE CHILD ALMOST HURT THE BIG DOG'])\n",
        "print(y1.shape)"
      ],
      "metadata": {
        "id": "F72pzl70fQIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d17a34d-4a91-4eef-9aab-0586cd1a26e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1)\n",
            "(1, 45920)\n",
            "(1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = wav2vec2Model(model)"
      ],
      "metadata": {
        "id": "xiDWQwPNMHR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbdddf2b-d8c6-437c-87c5-85f3f3ee851d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimator.device"
      ],
      "metadata": {
        "id": "g-ygIPMhRBwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae011f6a-0e4a-49cf-cd0c-f260927b8a53"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an adversarial attack using ImperceptibleASR method\n",
        "attack5 = ImperceptibleASRPyTorch(estimator=estimator, batch_size= 1, eps = 2.0)\n",
        "# Generate an adversarial example from the original audio data\n",
        "x_adv5 = attack5.generate(x=x, y=y1)"
      ],
      "metadata": {
        "id": "_pJ49c8lKCOC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7603b8f8-0d0a-497d-9a67-610feae0b088"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/adversarial-robustness-toolbox/art/estimators/speech_recognition/wav2vec2ModelWrapper.py:136: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(encoded_transcription, dtype=torch.long).to(self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss looks like this:  tensor(-35.2712, device='cuda:0') with type:  <class 'torch.Tensor'>\n",
            "Loss looks like this:  tensor(-35.2712, device='cuda:0') with type:  <class 'torch.Tensor'>\n",
            "Decoded output looks like this:  THE CHILD ALMOST URT THE SMALL DOG  with type:  <class 'numpy.ndarray'>\n",
            "masked adv input:  tensor([[-9.1553e-05, -9.1553e-05, -9.1553e-05,  ..., -5.3711e-03,\n",
            "         -5.1880e-03, -5.3711e-03]], device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<MulBackward0>) with type:  <class 'torch.Tensor'>\n",
            "original output:  ['THE CHILD ALMOST HURT THE BIG DOG'] with type:  <class 'numpy.ndarray'>\n",
            "real lengths:  [45920] with type:  <class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d425747a912f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mattack5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImperceptibleASRPyTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Generate an adversarial example from the original audio data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_adv5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/adversarial-robustness-toolbox/art/attacks/evasion/imperceptible_asr/imperceptible_asr_pytorch.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;31m# Then compute the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0madv_x_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_index_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_index_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/adversarial-robustness-toolbox/art/attacks/evasion/imperceptible_asr/imperceptible_asr_pytorch.py\u001b[0m in \u001b[0;36m_generate_batch\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# First stage of attack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0msuccessful_adv_input_1st_stage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attack_1st_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         successful_perturbation_1st_stage = successful_adv_input_1st_stage - torch.tensor(original_input).to(\n\u001b[1;32m    312\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/adversarial-robustness-toolbox/art/attacks/evasion/imperceptible_asr/imperceptible_asr_pytorch.py\u001b[0m in \u001b[0;36m_attack_1st_stage\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Get sign of the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the transcription of the original and adversarial audio data\n",
        "y_pred5 = estimator.predict( x )\n",
        "y_adv_pred5 = estimator.predict( x_adv5 )\n",
        "print(\"Original transcription:    \", y_pred5 )\n",
        "print(\"Adversarial transcription: \", y_adv_pred5 )"
      ],
      "metadata": {
        "id": "-ElGHtgoKJL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **---------------------------wav2vec try--------------------------------**"
      ],
      "metadata": {
        "id": "OrTCbBcvJ5Gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **------------------------------FAST GRADIENT DESCENT ATTACK-----------------------------**"
      ],
      "metadata": {
        "id": "UvcAa47Bg0hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an adversarial attack using ImperceptibleASR method\n",
        "attack1 = FastGradientMethod(estimator=estimator, batch_size= 1, eps = 0.3, targeted= True, eps_step= 0.01, norm = 2)\n",
        "# Generate an adversarial example from the original audio data\n",
        "x_adv1 = attack1.generate(x=x, y=y)"
      ],
      "metadata": {
        "id": "60AdIaMAghc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the transcription of the original and adversarial audio data\n",
        "y_pred1 = estimator.predict( x )\n",
        "y_adv_pred1 = estimator.predict( x_adv1 )\n",
        "print(\"Original transcription:    \", y_pred1 )\n",
        "print(\"Adversarial transcription: \", y_adv_pred1 )"
      ],
      "metadata": {
        "id": "KzW7r8jJfVcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean Audio**"
      ],
      "metadata": {
        "id": "vExNCMRBlfOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x, rate = sr)"
      ],
      "metadata": {
        "id": "f80ZvDailVOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perturbed Audio**"
      ],
      "metadata": {
        "id": "14mOMNWelhap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x_adv1, rate = sr)"
      ],
      "metadata": {
        "id": "PqP9Iw2ZVLUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **-----------------------------BASIC ITERATIVE METHOD ATTACK-----------------------------**"
      ],
      "metadata": {
        "id": "Dz-wFLB4exbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an adversarial attack using ImperceptibleASR method\n",
        "attack2 = BasicIterativeMethod(estimator=estimator, batch_size= 1, eps = 0.3, targeted= True, eps_step= 0.01, max_iter= 10, verbose= True)\n",
        "# Generate an adversarial example from the original audio data\n",
        "x_adv2 = attack2.generate(x=x, y=y)"
      ],
      "metadata": {
        "id": "CnZX69vwhOiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the transcription of the original and adversarial audio data\n",
        "y_pred2 = estimator.predict( x )\n",
        "y_adv_pred2 = estimator.predict( x_adv2 )\n",
        "print(\"Original transcription:    \", y_pred2 )\n",
        "print(\"Adversarial transcription: \", y_adv_pred2 )"
      ],
      "metadata": {
        "id": "L1Gb2eledWif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean Audio**"
      ],
      "metadata": {
        "id": "_IsDTeoplmFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x, rate = sr)"
      ],
      "metadata": {
        "id": "pXxBInHHlYxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perturbed Audio**"
      ],
      "metadata": {
        "id": "nNPDZfMGlmhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x_adv2, rate = sr)"
      ],
      "metadata": {
        "id": "aumrsEwolSvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--------------------------PROJECTED GRADIENT DESCENT ATTACK-------------------------**"
      ],
      "metadata": {
        "id": "xCwitokie9GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an adversarial attack using ImperceptibleASR method estimator=estimator, batch_size= 1, eps = 0.3, targeted= True, eps_step= 0.01,\n",
        "attack3 = ProjectedGradientDescent(estimator=estimator, batch_size= 1, eps = 0.3, targeted= True, eps_step= 0.01, max_iter= 10, verbose = True, norm = 2)\n",
        "# Generate an adversarial example from the original audio data\n",
        "x_adv3 = attack1.generate(x=x, y=y)"
      ],
      "metadata": {
        "id": "SYC3JZ96hRui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the transcription of the original and adversarial audio data\n",
        "y_pred3 = estimator.predict( x )\n",
        "y_adv_pred3 = estimator.predict( x_adv3 )\n",
        "print(\"Original transcription:    \", y_pred3 )\n",
        "print(\"Adversarial transcription: \", y_adv_pred3)"
      ],
      "metadata": {
        "id": "q_obECXEfCnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean Audio**"
      ],
      "metadata": {
        "id": "D-pBcuXYlqCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x, rate = sr)"
      ],
      "metadata": {
        "id": "vg4-d2tslZeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perturbed Audio**"
      ],
      "metadata": {
        "id": "gnAcekE-lpZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x_adv3, rate = sr)"
      ],
      "metadata": {
        "id": "I8Ni5WEmlz9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--------------------------------CARLINI AND WAGNER ATTACK-------------------------------**"
      ],
      "metadata": {
        "id": "Q0BWXs7RgG3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an adversarial attack using ImperceptibleASR method\n",
        "attack4 = CarliniWagnerASR(estimator=estimator, batch_size= 1, eps = 2.0, decrease_factor_eps = 10.0)\n",
        "# Generate an adversarial example from the original audio data\n",
        "x_adv4 = attack4.generate(x=x, y=y1)"
      ],
      "metadata": {
        "id": "62ILflWOhU7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the transcription of the original and adversarial audio data\n",
        "y_pred4 = estimator.predict( x )\n",
        "y_adv_pred4 = estimator.predict( x_adv4 )\n",
        "print(\"Original transcription: \", y_pred4 )\n",
        "print(\"Adversarial transcription: \", y_adv_pred4 )"
      ],
      "metadata": {
        "id": "7Z7GtpFGW5ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean Audio**"
      ],
      "metadata": {
        "id": "4EQMq4salsfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x, rate = sr)"
      ],
      "metadata": {
        "id": "R6i7jsdaldAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perturbed Audio**"
      ],
      "metadata": {
        "id": "oFR_Tk-qlr5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x_adv4, rate = sr)"
      ],
      "metadata": {
        "id": "d1Gbf6e2W7E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **---------------------------------IMPERCEPTIBLE ASR ATTACK--------------------------------**"
      ],
      "metadata": {
        "id": "te_nHK8dga4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOCUMENTATION OF IMPERCEPTABLE ASR ATTACK\n",
        "\n",
        "**ARGUMENTS:** estimator: SPEECH_RECOGNIZER_TYPE, masker: PsychoacousticMasker, eps: float = 2000.0, learning_rate_1: float = 100.0, max_iter_1: int = 1000, alpha: float = 0.05, learning_rate_2: float = 1.0, max_iter_2: int = 4000, loss_theta_min: float = 0.05, decrease_factor_eps: float = 0.8, num_iter_decrease_eps: int = 10, increase_factor_alpha: float = 1.2, num_iter_increase_alpha: int = 20, decrease_factor_alpha: float = 0.8, num_iter_decrease_alpha: int = 50, batch_size: int = 1)→ None¶\n",
        "Create an instance of the ImperceptibleASR.\n",
        "\n",
        "The default parameters assume that audio input is in int16 range. If using normalized audio input, parameters eps and learning_rate_{1,2} need to be scaled with a factor 2^-15\n",
        "\n",
        "Parameters\n",
        ":\n",
        "estimator – A trained speech recognition estimator.\n",
        "\n",
        "masker – A Psychoacoustic masker.\n",
        "\n",
        "eps (float) – Initial max norm bound for adversarial perturbation.\n",
        "\n",
        "learning_rate_1 (float) – Learning rate for stage 1 of attack.\n",
        "\n",
        "max_iter_1 (int) – Number of iterations for stage 1 of attack.\n",
        "\n",
        "alpha (float) – Initial alpha value for balancing stage 2 loss.\n",
        "\n",
        "learning_rate_2 (float) – Learning rate for stage 2 of attack.\n",
        "\n",
        "max_iter_2 (int) – Number of iterations for stage 2 of attack.\n",
        "\n",
        "loss_theta_min (float) – If imperceptible loss reaches minimum, stop early. Works best with batch_size=1.\n",
        "\n",
        "decrease_factor_eps (float) – Decrease factor for epsilon (Paper default: 0.8).\n",
        "\n",
        "num_iter_decrease_eps (int) – Iterations after which to decrease epsilon if attack succeeds (Paper default: 10).\n",
        "\n",
        "increase_factor_alpha (float) – Increase factor for alpha (Paper default: 1.2).\n",
        "\n",
        "num_iter_increase_alpha (int) – Iterations after which to increase alpha if attack succeeds (Paper default: 20).\n",
        "\n",
        "decrease_factor_alpha (float) – Decrease factor for alpha (Paper default: 0.8).\n",
        "\n",
        "num_iter_decrease_alpha (int) – Iterations after which to decrease alpha if attack fails (Paper default: 50).\n",
        "\n",
        "batch_size (int) – Batch size."
      ],
      "metadata": {
        "id": "FxhkzhfG33Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an adversarial attack using ImperceptibleASR method\n",
        "attack5 = ImperceptibleASRPyTorch(estimator=estimator, batch_size= 1, eps = 2.0)\n",
        "# Generate an adversarial example from the original audio data\n",
        "x_adv5 = attack5.generate(x=x, y=y1)"
      ],
      "metadata": {
        "id": "QU2-7TvPgWMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the transcription of the original and adversarial audio data\n",
        "y_pred5 = estimator.predict( x )\n",
        "y_adv_pred5 = estimator.predict( x_adv5 )\n",
        "print(\"Original transcription:    \", y_pred5 )\n",
        "print(\"Adversarial transcription: \", y_adv_pred5 )"
      ],
      "metadata": {
        "id": "3ifQLDGihfbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean Audio**"
      ],
      "metadata": {
        "id": "7ThaAnP3lurJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x, rate = sr)"
      ],
      "metadata": {
        "id": "EbtgMQ26lbRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perturbed Audio**"
      ],
      "metadata": {
        "id": "5H9UeXI-luSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(x_adv5, rate = sr)"
      ],
      "metadata": {
        "id": "0O62uxmKhhAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **----------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "9i0Zi055mW8K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8gOPiupR6jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  pass"
      ],
      "metadata": {
        "id": "R6gtiuqCbmbm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}